{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], d_loss: 0.3770, g_loss: 3.0416\n",
      "Epoch [20/100], d_loss: 0.2656, g_loss: 4.1275\n",
      "Epoch [30/100], d_loss: 0.2642, g_loss: 4.7423\n",
      "Epoch [40/100], d_loss: 0.1993, g_loss: 5.3193\n",
      "Epoch [50/100], d_loss: 0.2050, g_loss: 5.7866\n",
      "Epoch [60/100], d_loss: 0.1557, g_loss: 6.3521\n",
      "Epoch [70/100], d_loss: 0.1675, g_loss: 6.4343\n",
      "Epoch [80/100], d_loss: 0.1745, g_loss: 6.7328\n",
      "Epoch [90/100], d_loss: 0.1348, g_loss: 7.1126\n",
      "Epoch [100/100], d_loss: 0.1672, g_loss: 7.2631\n",
      "Synthetic and merged datasets have been saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "class CovidDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = torch.FloatTensor(data.values)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Linear(1024, output_dim),\n",
    "            nn.Sigmoid()  # Use Sigmoid to output values between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "def train_gan(data_path, num_epochs=500, batch_size=32, latent_dim=200):\n",
    "    # Load and prepare data\n",
    "    df = pd.read_csv(data_path)\n",
    "\n",
    "    # Convert all columns to numeric, forcing errors to NaN\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Drop rows with NaN values\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Normalize the data\n",
    "    scaler = MinMaxScaler()\n",
    "    df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "    dataset = CovidDataset(df_scaled)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    generator = Generator(latent_dim, len(df.columns))\n",
    "    discriminator = Discriminator(len(df.columns))\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    g_optimizer = optim.Adam(generator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
    "    d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_g_loss = 0\n",
    "        epoch_d_loss = 0\n",
    "        batches = 0\n",
    "\n",
    "        for i, real_data in enumerate(dataloader):\n",
    "            batch_size = real_data.size(0)\n",
    "            batches += 1\n",
    "\n",
    "            # Train Discriminator\n",
    "            d_optimizer.zero_grad()\n",
    "            label_real = torch.ones(batch_size, 1)\n",
    "            label_fake = torch.zeros(batch_size, 1)\n",
    "\n",
    "            z = torch.randn(batch_size, latent_dim)\n",
    "            fake_data = generator(z)\n",
    "\n",
    "            d_loss_real = criterion(discriminator(real_data), label_real)\n",
    "            d_loss_fake = criterion(discriminator(fake_data.detach()), label_fake)\n",
    "            d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "            d_loss.backward()\n",
    "            d_optimizer.step()\n",
    "\n",
    "            # Train Generator\n",
    "            g_optimizer.zero_grad()\n",
    "            z = torch.randn(batch_size, latent_dim)\n",
    "            fake_data = generator(z)\n",
    "            g_loss = criterion(discriminator(fake_data), label_real)\n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "\n",
    "            epoch_g_loss += g_loss.item()\n",
    "            epoch_d_loss += d_loss.item()\n",
    "\n",
    "        avg_g_loss = epoch_g_loss / batches\n",
    "        avg_d_loss = epoch_d_loss / batches\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], d_loss: {avg_d_loss:.4f}, g_loss: {avg_g_loss:.4f}')\n",
    "\n",
    "    return generator, scaler\n",
    "\n",
    "def generate_samples(generator, scaler, num_samples=100, latent_dim=200):\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(num_samples, latent_dim)\n",
    "        generated_data = generator(z).numpy()\n",
    "\n",
    "    # Inverse transform the generated data\n",
    "    df_generated = pd.DataFrame(scaler.inverse_transform(generated_data), columns=scaler.feature_names_in_)\n",
    "\n",
    "    # Round the generated data to get binary values (0 or 1)\n",
    "    df_generated = df_generated.round().clip(0, 1)  # Ensure values are either 0 or 1\n",
    "    return df_generated\n",
    "\n",
    "def main():\n",
    "    data_path = '/Users/ishaanpothapragada/Desktop/Desktop_Documents/Maize/encoded_data.csv'  # Update this path\n",
    "\n",
    "    generator, scaler = train_gan(data_path, num_epochs=100, latent_dim=200)\n",
    "\n",
    "    original_data = pd.read_csv(data_path)\n",
    "\n",
    "    # Generate synthetic samples\n",
    "    synthetic_data = generate_samples(generator, scaler, num_samples=20000)  # Increase the number of samples\n",
    "\n",
    "    # Ensure the synthetic data has the same columns as the original data\n",
    "    synthetic_data.columns = original_data.columns\n",
    "\n",
    "    # Add source column\n",
    "    original_data['data_source'] = 'original'\n",
    "    synthetic_data['data_source'] = 'synthetic'\n",
    "\n",
    "    # Merge datasets\n",
    "    merged_data = pd.concat([original_data, synthetic_data], axis=0, ignore_index=True)\n",
    "\n",
    "    # Save datasets\n",
    "    synthetic_data.to_csv('synthetic_covid_data.csv', index=False)\n",
    "    merged_data.to_csv('merged_covid_data.csv', index=False)\n",
    "\n",
    "    print(\"Synthetic and merged datasets have been saved.\")\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/500], d_loss: 0.3709, g_loss: 3.1858\n",
      "Epoch [20/500], d_loss: 0.2807, g_loss: 3.8733\n",
      "Epoch [30/500], d_loss: 0.2380, g_loss: 4.5520\n",
      "Epoch [40/500], d_loss: 0.1949, g_loss: 5.2693\n",
      "Epoch [50/500], d_loss: 0.1686, g_loss: 5.8676\n",
      "Epoch [60/500], d_loss: 0.2153, g_loss: 6.1037\n",
      "Epoch [70/500], d_loss: 0.1723, g_loss: 6.2785\n",
      "Epoch [80/500], d_loss: 0.1533, g_loss: 6.2547\n",
      "Epoch [90/500], d_loss: 0.1852, g_loss: 6.2511\n",
      "Epoch [100/500], d_loss: 0.1696, g_loss: 6.2088\n",
      "Epoch [110/500], d_loss: 0.1840, g_loss: 6.2395\n",
      "Epoch [120/500], d_loss: 0.1730, g_loss: 6.2680\n",
      "Epoch [130/500], d_loss: 0.1543, g_loss: 6.1716\n",
      "Epoch [140/500], d_loss: 0.1573, g_loss: 6.2521\n",
      "Epoch [150/500], d_loss: 0.1468, g_loss: 6.0427\n",
      "Epoch [160/500], d_loss: 0.1505, g_loss: 5.7467\n",
      "Epoch [170/500], d_loss: 0.1621, g_loss: 5.7485\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 164\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSynthetic and merged datasets have been saved.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m: \n\u001b[1;32m--> 164\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 140\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m    138\u001b[0m     data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoded_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m# Update this path\u001b[39;00m\n\u001b[1;32m--> 140\u001b[0m     generator, scaler \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_gan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatent_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m     original_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(data_path)\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;66;03m# Generate synthetic samples\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 109\u001b[0m, in \u001b[0;36mtrain_gan\u001b[1;34m(data_path, num_epochs, batch_size, latent_dim)\u001b[0m\n\u001b[0;32m    107\u001b[0m g_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    108\u001b[0m z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(batch_size, latent_dim)\n\u001b[1;32m--> 109\u001b[0m fake_data \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m g_loss \u001b[38;5;241m=\u001b[39m criterion(discriminator(fake_data), label_real)\n\u001b[0;32m    111\u001b[0m g_loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\aurok\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\aurok\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[1], line 38\u001b[0m, in \u001b[0;36mGenerator.forward\u001b[1;34m(self, z)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, z):\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aurok\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\aurok\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\aurok\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\aurok\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\aurok\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\aurok\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "class CovidDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = torch.FloatTensor(data.values)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Linear(1024, output_dim),\n",
    "            nn.Sigmoid()  # Use Sigmoid to output values between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "def train_gan(data_path, num_epochs=500, batch_size=32, latent_dim=200):\n",
    "    # Load and prepare data\n",
    "    df = pd.read_csv(data_path)\n",
    "\n",
    "    # Convert all columns to numeric, forcing errors to NaN\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Drop rows with NaN values\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Normalize the data\n",
    "    scaler = MinMaxScaler()\n",
    "    df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "    dataset = CovidDataset(df_scaled)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    generator = Generator(latent_dim, len(df.columns))\n",
    "    discriminator = Discriminator(len(df.columns))\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    g_optimizer = optim.Adam(generator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
    "    d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_g_loss = 0\n",
    "        epoch_d_loss = 0\n",
    "        batches = 0\n",
    "\n",
    "        for i, real_data in enumerate(dataloader):\n",
    "            batch_size = real_data.size(0)\n",
    "            batches += 1\n",
    "\n",
    "            # Train Discriminator\n",
    "            d_optimizer.zero_grad()\n",
    "            label_real = torch.ones(batch_size, 1)\n",
    "            label_fake = torch.zeros(batch_size, 1)\n",
    "\n",
    "            z = torch.randn(batch_size, latent_dim)\n",
    "            fake_data = generator(z)\n",
    "\n",
    "            d_loss_real = criterion(discriminator(real_data), label_real)\n",
    "            d_loss_fake = criterion(discriminator(fake_data.detach()), label_fake)\n",
    "            d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "            d_loss.backward()\n",
    "            d_optimizer.step()\n",
    "\n",
    "            # Train Generator\n",
    "            g_optimizer.zero_grad()\n",
    "            z = torch.randn(batch_size, latent_dim)\n",
    "            fake_data = generator(z)\n",
    "            g_loss = criterion(discriminator(fake_data), label_real)\n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "\n",
    "            epoch_g_loss += g_loss.item()\n",
    "            epoch_d_loss += d_loss.item()\n",
    "\n",
    "        avg_g_loss = epoch_g_loss / batches\n",
    "        avg_d_loss = epoch_d_loss / batches\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], d_loss: {avg_d_loss:.4f}, g_loss: {avg_g_loss:.4f}')\n",
    "\n",
    "    return generator, scaler\n",
    "\n",
    "def generate_samples(generator, scaler, num_samples=100, latent_dim=200):\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(num_samples, latent_dim)\n",
    "        generated_data = generator(z).numpy()\n",
    "\n",
    "    # Inverse transform the generated data\n",
    "    df_generated = pd.DataFrame(scaler.inverse_transform(generated_data), columns=scaler.feature_names_in_)\n",
    "\n",
    "    # Round the generated data to get binary values (0 or 1)\n",
    "    df_generated = df_generated.round().clip(0, 1)  # Ensure values are either 0 or 1\n",
    "    return df_generated\n",
    "\n",
    "def main():\n",
    "    data_path = 'encoded_data.csv' # Update this path\n",
    "\n",
    "    generator, scaler = train_gan(data_path, num_epochs=500, latent_dim=200)\n",
    "\n",
    "    original_data = pd.read_csv(data_path)\n",
    "\n",
    "    # Generate synthetic samples\n",
    "    synthetic_data = generate_samples(generator, scaler, num_samples=20000)  # Increase the number of samples\n",
    "\n",
    "    # Ensure the synthetic data has the same columns as the original data\n",
    "    synthetic_data.columns = original_data.columns\n",
    "\n",
    "    # Add source column\n",
    "    original_data['data_source'] = 'original'\n",
    "    synthetic_data['data_source'] = 'synthetic'\n",
    "\n",
    "    # Merge datasets\n",
    "    merged_data = pd.concat([original_data, synthetic_data], axis=0, ignore_index=True)\n",
    "\n",
    "    # Save datasets\n",
    "    synthetic_data.to_csv('synthetic500_covid_data.csv', index=False)\n",
    "    merged_data.to_csv('merged500_covid_data.csv', index=False)\n",
    "\n",
    "    print(\"Synthetic and merged datasets have been saved.\")\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\aurok\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\aurok\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: torch in c:\\users\\aurok\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.5.1+cu118)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\aurok\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\aurok\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.13.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\aurok\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\aurok\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\aurok\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\aurok\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\aurok\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\aurok\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\aurok\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\aurok\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (2024.2.0)\n",
      "INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
      "\n",
      "The conflict is caused by:\n",
      "    The user requested sympy\n",
      "    torch 2.5.1+cu118 depends on sympy==1.13.1; python_version >= \"3.9\"\n",
      "\n",
      "To fix this you could try to:\n",
      "1. loosen the range of package versions you've specified\n",
      "2. remove package versions to allow pip to attempt to solve the dependency conflict\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000002B22E39B730>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/pandas/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000002B22E39BA60>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/pandas/\n",
      "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000002B22E39BC10>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/pandas/\n",
      "WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000002B22E39BDC0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/pandas/\n",
      "WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000002B22E39BFA0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/pandas/\n",
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000002B22E3E4C40>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/numpy/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000002B22E3E4FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/numpy/\n",
      "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000002B22E3E5180>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/numpy/\n",
      "WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000002B22E3E5330>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/numpy/\n",
      "WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000002B22E3E54E0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/numpy/\n",
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000002B22E3E56F0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/torch/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000002B22E3E5990>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/torch/\n",
      "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000002B22E3E5B40>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/torch/\n",
      "WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000002B22E3E5CF0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/torch/\n",
      "WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000002B22E3E5EA0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/torch/\n",
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000002B22E3E65C0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/scikit-learn/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000002B22E3E6950>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/scikit-learn/\n",
      "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000002B22E3E6B00>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/scikit-learn/\n",
      "WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000002B22E3E6CB0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/scikit-learn/\n",
      "WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000002B22E3E6E90>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/scikit-learn/\n",
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000002B22E3E7520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/sympy/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000002B22E3E6830>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/sympy/\n",
      "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000002B22E3E6E00>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/sympy/\n",
      "WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000002B22E3E6BC0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/sympy/\n",
      "WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000002B22E3E69B0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/sympy/\n",
      "ERROR: Cannot install sympy and torch==2.5.1+cu118 because these package versions have conflicting dependencies.\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --upgrade pandas numpy torch scikit-learn sympy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\aurok\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\aurok\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: torch in c:\\users\\aurok\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.5.1+cu118)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\aurok\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\aurok\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\aurok\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\aurok\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\aurok\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\aurok\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\aurok\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\aurok\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\aurok\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\aurok\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\aurok\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\aurok\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\aurok\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\aurok\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aurok\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\aurok\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch) (2.1.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pandas numpy torch scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: sympy 1.13.1\n",
      "Uninstalling sympy-1.13.1:\n",
      "  Successfully uninstalled sympy-1.13.1\n",
      "Collecting sympy\n",
      "  Downloading sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\aurok\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy) (1.3.0)\n",
      "Downloading sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.2 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.5/6.2 MB 40.8 kB/s eta 0:02:19\n",
      "   --- ------------------------------------ 0.5/6.2 MB 40.8 kB/s eta 0:02:19\n",
      "   ----- ---------------------------------- 0.8/6.2 MB 77.3 kB/s eta 0:01:10\n",
      "   ------ --------------------------------- 1.0/6.2 MB 111.9 kB/s eta 0:00:46\n",
      "   ------ --------------------------------- 1.0/6.2 MB 111.9 kB/s eta 0:00:46\n",
      "   ------ --------------------------------- 1.0/6.2 MB 111.9 kB/s eta 0:00:46\n",
      "   ------ --------------------------------- 1.0/6.2 MB 111.9 kB/s eta 0:00:46\n",
      "   ------ --------------------------------- 1.0/6.2 MB 111.9 kB/s eta 0:00:46\n",
      "   -------- ------------------------------- 1.3/6.2 MB 131.3 kB/s eta 0:00:38\n",
      "   -------- ------------------------------- 1.3/6.2 MB 131.3 kB/s eta 0:00:38\n",
      "   -------- ------------------------------- 1.3/6.2 MB 131.3 kB/s eta 0:00:38\n",
      "   ---------- ----------------------------- 1.6/6.2 MB 151.1 kB/s eta 0:00:31\n",
      "   ----------- ---------------------------- 1.8/6.2 MB 175.1 kB/s eta 0:00:25\n",
      "   ----------- ---------------------------- 1.8/6.2 MB 175.1 kB/s eta 0:00:25\n",
      "   ------------- -------------------------- 2.1/6.2 MB 197.4 kB/s eta 0:00:21\n",
      "   ------------- -------------------------- 2.1/6.2 MB 197.4 kB/s eta 0:00:21\n",
      "   ------------- -------------------------- 2.1/6.2 MB 197.4 kB/s eta 0:00:21\n",
      "   ------------- -------------------------- 2.1/6.2 MB 197.4 kB/s eta 0:00:21\n",
      "   --------------- ------------------------ 2.4/6.2 MB 205.2 kB/s eta 0:00:19\n",
      "   --------------- ------------------------ 2.4/6.2 MB 205.2 kB/s eta 0:00:19\n",
      "   --------------- ------------------------ 2.4/6.2 MB 205.2 kB/s eta 0:00:19\n",
      "   --------------- ------------------------ 2.4/6.2 MB 205.2 kB/s eta 0:00:19\n",
      "   --------------- ------------------------ 2.4/6.2 MB 205.2 kB/s eta 0:00:19\n",
      "   --------------- ------------------------ 2.4/6.2 MB 205.2 kB/s eta 0:00:19\n",
      "   --------------- ------------------------ 2.4/6.2 MB 205.2 kB/s eta 0:00:19\n",
      "   --------------- ------------------------ 2.4/6.2 MB 205.2 kB/s eta 0:00:19\n",
      "   ---------------- ----------------------- 2.6/6.2 MB 199.2 kB/s eta 0:00:18\n",
      "   ---------------- ----------------------- 2.6/6.2 MB 199.2 kB/s eta 0:00:18\n",
      "   -------------------- ------------------- 3.1/6.2 MB 233.3 kB/s eta 0:00:14\n",
      "   ---------------------- ----------------- 3.4/6.2 MB 251.0 kB/s eta 0:00:12\n",
      "   ----------------------- ---------------- 3.7/6.2 MB 268.6 kB/s eta 0:00:10\n",
      "   ------------------------- -------------- 3.9/6.2 MB 285.4 kB/s eta 0:00:08\n",
      "   --------------------------- ------------ 4.2/6.2 MB 299.6 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 4.2/6.2 MB 299.6 kB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 4.5/6.2 MB 307.8 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 4.7/6.2 MB 322.6 kB/s eta 0:00:05\n",
      "   ------------------------------ --------- 4.7/6.2 MB 322.6 kB/s eta 0:00:05\n",
      "   -------------------------------- ------- 5.0/6.2 MB 333.0 kB/s eta 0:00:04\n",
      "   -------------------------------- ------- 5.0/6.2 MB 333.0 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 5.2/6.2 MB 342.8 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 5.5/6.2 MB 355.1 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 5.8/6.2 MB 365.1 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 5.8/6.2 MB 365.1 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 5.8/6.2 MB 365.1 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 5.8/6.2 MB 365.1 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 5.8/6.2 MB 365.1 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 5.8/6.2 MB 365.1 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 5.8/6.2 MB 365.1 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 5.8/6.2 MB 365.1 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 6.0/6.2 MB 345.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.2/6.2 MB 349.0 kB/s eta 0:00:00\n",
      "Installing collected packages: sympy\n",
      "Successfully installed sympy-1.13.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torch 2.5.1+cu118 requires sympy==1.13.1; python_version >= \"3.9\", but you have sympy 1.13.3 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip3 uninstall sympy --y\n",
    "!pip3 install sympy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
